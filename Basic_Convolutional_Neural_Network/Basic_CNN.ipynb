{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28 # Figs size 28*28\n",
    "num_classes=10 # Numbers of labels\n",
    "num_epochs=3 # Training iteration\n",
    "batch_size=64 # 64 Figs\n",
    "\n",
    "# Dataset from MNIST\n",
    "train_dataset=datasets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset=datasets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,          # Gray Scale Image\n",
    "                out_channels=16,        # Output\n",
    "                kernel_size=5,          # Kernel size\n",
    "                stride=1,               # step\n",
    "                padding=2,              # Added side\n",
    "            ),                          # Output (16,28,28)\n",
    "            nn.ReLU() ,                 # ReLU\n",
    "            nn.MaxPool2d(kernel_size=2),# Max Pool output (16,14,14)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(       # Input(16,14,14)\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,         # Gray Scale Image\n",
    "                out_channels=32,        # Output\n",
    "                kernel_size=5,          # Kernel size\n",
    "                stride=1,               # step\n",
    "                padding=2,              # Added side\n",
    "            ),                          # Output (32,14,14)\n",
    "            nn.ReLU(),                  # ReLU\n",
    "            nn.MaxPool2d(kernel_size=2),# Max Pool output (32,7,7)\n",
    "        )\n",
    "        self.out=nn.Linear(in_features=32*7*7,out_features=10) # Fully connected layers\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=x.view(x.size(0),-1)          # Flatten\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred = torch.max(predictions.data,1)[1]\n",
    "    rights =pred.eq(labels.data.view_as(pred)).sum()\n",
    "    return rights,len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [0/60000(0%)]\tloss: 2.318325\tTrain set accuracy: 7.81%\tTest set accuracy: 15.91%\n",
      "epoch: 0 [6400/60000(11%)]\tloss: 0.425031\tTrain set accuracy: 72.88%\tTest set accuracy: 92.19%\n",
      "epoch: 0 [12800/60000(21%)]\tloss: 0.089556\tTrain set accuracy: 83.22%\tTest set accuracy: 95.39%\n",
      "epoch: 0 [19200/60000(32%)]\tloss: 0.067831\tTrain set accuracy: 87.29%\tTest set accuracy: 96.33%\n",
      "epoch: 0 [25600/60000(43%)]\tloss: 0.035491\tTrain set accuracy: 89.59%\tTest set accuracy: 96.10%\n",
      "epoch: 0 [32000/60000(53%)]\tloss: 0.021460\tTrain set accuracy: 91.05%\tTest set accuracy: 97.53%\n",
      "epoch: 0 [38400/60000(64%)]\tloss: 0.077475\tTrain set accuracy: 92.10%\tTest set accuracy: 97.73%\n",
      "epoch: 0 [44800/60000(75%)]\tloss: 0.038107\tTrain set accuracy: 92.85%\tTest set accuracy: 97.62%\n",
      "epoch: 0 [51200/60000(85%)]\tloss: 0.077965\tTrain set accuracy: 93.43%\tTest set accuracy: 98.08%\n",
      "epoch: 0 [57600/60000(96%)]\tloss: 0.015774\tTrain set accuracy: 93.92%\tTest set accuracy: 97.93%\n",
      "epoch: 1 [0/60000(0%)]\tloss: 0.047839\tTrain set accuracy: 98.44%\tTest set accuracy: 98.07%\n",
      "epoch: 1 [6400/60000(11%)]\tloss: 0.032058\tTrain set accuracy: 98.21%\tTest set accuracy: 98.33%\n",
      "epoch: 1 [12800/60000(21%)]\tloss: 0.080202\tTrain set accuracy: 98.16%\tTest set accuracy: 98.38%\n",
      "epoch: 1 [19200/60000(32%)]\tloss: 0.056470\tTrain set accuracy: 98.21%\tTest set accuracy: 98.50%\n",
      "epoch: 1 [25600/60000(43%)]\tloss: 0.035072\tTrain set accuracy: 98.18%\tTest set accuracy: 98.33%\n",
      "epoch: 1 [32000/60000(53%)]\tloss: 0.036303\tTrain set accuracy: 98.20%\tTest set accuracy: 98.54%\n",
      "epoch: 1 [38400/60000(64%)]\tloss: 0.053185\tTrain set accuracy: 98.21%\tTest set accuracy: 98.40%\n",
      "epoch: 1 [44800/60000(75%)]\tloss: 0.045253\tTrain set accuracy: 98.18%\tTest set accuracy: 98.62%\n",
      "epoch: 1 [51200/60000(85%)]\tloss: 0.005481\tTrain set accuracy: 98.19%\tTest set accuracy: 98.56%\n",
      "epoch: 1 [57600/60000(96%)]\tloss: 0.024798\tTrain set accuracy: 98.24%\tTest set accuracy: 98.72%\n",
      "epoch: 2 [0/60000(0%)]\tloss: 0.185890\tTrain set accuracy: 96.88%\tTest set accuracy: 98.64%\n",
      "epoch: 2 [6400/60000(11%)]\tloss: 0.069316\tTrain set accuracy: 98.56%\tTest set accuracy: 98.58%\n",
      "epoch: 2 [12800/60000(21%)]\tloss: 0.032346\tTrain set accuracy: 98.56%\tTest set accuracy: 98.71%\n",
      "epoch: 2 [19200/60000(32%)]\tloss: 0.032949\tTrain set accuracy: 98.61%\tTest set accuracy: 98.48%\n",
      "epoch: 2 [25600/60000(43%)]\tloss: 0.073369\tTrain set accuracy: 98.65%\tTest set accuracy: 98.84%\n",
      "epoch: 2 [32000/60000(53%)]\tloss: 0.064491\tTrain set accuracy: 98.69%\tTest set accuracy: 98.74%\n",
      "epoch: 2 [38400/60000(64%)]\tloss: 0.014010\tTrain set accuracy: 98.71%\tTest set accuracy: 98.68%\n",
      "epoch: 2 [44800/60000(75%)]\tloss: 0.042699\tTrain set accuracy: 98.70%\tTest set accuracy: 98.55%\n",
      "epoch: 2 [51200/60000(85%)]\tloss: 0.009528\tTrain set accuracy: 98.71%\tTest set accuracy: 98.53%\n",
      "epoch: 2 [57600/60000(96%)]\tloss: 0.103988\tTrain set accuracy: 98.73%\tTest set accuracy: 98.77%\n"
     ]
    }
   ],
   "source": [
    "net = CNN()\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_rights=[]\n",
    "\n",
    "    for batch_idx,(data,target) in enumerate(train_loader): # Iterration for each batch\n",
    "        net.train()\n",
    "        output=net(data)\n",
    "        loss=criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        right=accuracy(output,target)\n",
    "        train_rights.append(right)\n",
    "\n",
    "        if batch_idx % 100==0:\n",
    "            net.eval()\n",
    "            val_rights=[]\n",
    "            for (data,target) in test_loader:\n",
    "                output=net(data)\n",
    "                right=accuracy(output,target)\n",
    "                val_rights.append(right)\n",
    "            # Caculate accuracy score\n",
    "            train_r=(sum([tup[0] for tup in train_rights]),sum([tup[1] for tup in train_rights]))\n",
    "            val_r=(sum([tup[0] for tup in val_rights]),sum([tup[1] for tup in val_rights]))\n",
    "            print('epoch: {} [{}/{}({:.0f}%)]\\tloss: {:.6f}\\tTrain set accuracy: {:.2f}%\\tTest set accuracy: {:.2f}%'.format(\n",
    "            epoch,batch_idx*batch_size,len(train_loader.dataset),100.*batch_idx/len(train_loader),loss.data,100.*train_r[0].numpy()/train_r[1],100.*val_r[0].numpy()/val_r[1]\n",
    "            ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
